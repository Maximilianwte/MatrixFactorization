{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.sparse as sparse\nimport os\nimport implicit\nimport pickle\nfrom multiprocessing import Pool","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Implicit:\n    def __init__(self, seed = 941, environment = \"offline\"):\n        self.seed = seed\n        \n        if environment == \"kaggle\":\n            self.read_matrices_timesplit(\"/kaggle/input/\")\n        else:\n            self.read_matrices_timesplit(\"../\")\n        \n    def read_matrices_timesplit(self, path):\n        self.training_set_csr = sparse.load_npz(f\"{path}/ratings_matrix_csr.npz\")\n        self.training_set_coo = sparse.load_npz(f\"{path}/ratings_matrix_coo.npz\")\n        \n        self.user_ids = np.load(f\"{path}/user_ids.npy\")\n        self.content_ids = np.load(f\"{path}/content_ids.npy\")\n        \n        self.test_set = pd.read_pickle(f\"{path}/test_set.pkl\", compression=\"zip\")\n\n        self.num_users, self.num_items = self.training_set_csr.shape\n    \n    def alternating_least_squares(self, iterations, factors, i_lambda, alpha):\n        self.model = implicit.als.AlternatingLeastSquares(factors=factors, regularization = i_lambda, iterations = iterations)\n        self.model.fit((self.training_set_csr*alpha).astype('double'))\n                \n    def logistic_factorization(self, iterations, factors, i_lambda, learning_parameter):\n        self.model = implicit.lmf.LogisticMatrixFactorization(factors, learning_parameter, i_lambda, iterations = iterations)\n        self.model.fit((self.training_set_coo).astype('double'))\n    \n    def predict_user_byIndex(self, index):\n        return self.model.item_factors[index].dot(self.model.user_factors.T)\n    \n    def get_user_vectors(self, users):\n        user_dict = {\"user\": \"values\"}\n        \n        for i in range(len(users)):\n            try:\n                user_indx = np.where(self.user_ids==users[i])[0][0]\n                user_dict[users[i]] = self.predict_user_byIndex(user_indx)\n            except:\n                self.not_testable += 1\n                \n        return user_dict\n    \n    def get_rank(self, user, item, user_vector = None):\n        # If a user_vector is specificied as input, the calculation is done before.\n        \n        user_indx = np.where(self.user_ids==user)[0][0]\n        item_indx = np.where(self.content_ids==item)[0][0]\n        \n        if user_vector == None:\n            user_vector = self.predict_user_byIndex(user_indx)\n            \n        prob = user_vector[item_indx]\n        \n        return np.where(np.sort(user_vector, kind=\"mergesort\")[::-1]==prob)[0][0]\n    \n    def expected_percentile_ranking(self):\n        accuracy = list()\n        mar = 0   \n        self.not_testable = 0        \n        users = self.test_set.idUser.values\n        contents = self.test_set.fullId.values\n        breakpoint = 4000\n        reportpoint = breakpoint if breakpoint < len(self.test_set.idUser) else len(self.test_set.idUser)\n        step = int(len(self.test_set.idUser) / 12) if breakpoint == None else int(breakpoint / 16.5)\n        \n        user_dict = self.get_user_vectors(self.test_set.idUser.unique())\n        \n        for i in range(len(self.test_set.idUser)):\n            if i > breakpoint:\n                break\n                \n            try:\n                accuracy.append(self.get_rank(users[i], contents[i], user_vector = user_dict[users[i]]) / self.num_items)\n            except:\n                pass\n                \n            if i % step == 0:\n                print(f\"Solved iteration: {i}. That's about {np.round((i/reportpoint)*100,2)}%.\")\n            \n        mar = np.mean(accuracy)\n            \n        return mar, accuracy","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Studio:\n    def __init__(self, seed, env, factorizer = \"\"):\n        self.counter = 0\n        self.seed = seed\n        self.env = env\n        \n        if factorizer == \"\":\n            self.MF = Implicit(seed, env)\n            self.MF.create_training_data()\n        else:\n            self.MF = factorizer\n            \n    def save_model(self, user_vec, item_vec, parameters = \"\"):\n        np.save(f\"user_vec_{self.counter}_{parameters}\", user_vec)\n        np.save(f\"item_vec_{self.counter}_{parameters}\", item_vec)\n        self.counter += 1\n        \n    def run_test_als(self, v_iterations, v_factors, v_lambdas, v_alphas):\n        model_acc = list()\n        \n        for it in v_iterations:\n            for factor in v_factors:\n                for in_lambda in v_lambdas:\n                    for alpha in v_alphas:\n                        print(f\"iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\")\n                        self.MF.alternating_least_squares(iterations = it, factors = factor, i_lambda = in_lambda, alpha = alpha)\n                        mar, accuracy = self.MF.expected_percentile_ranking()\n                        \n                        self.save_model(self.MF.model.user_factors, self.MF.model.item_factors, f\"iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\")\n                        model_acc.append([f\"model_als-iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\", mar, accuracy])\n                        pickle.dump(model_acc,open(\"model_acc\",\"w\"))\n                        \n        return model_acc\n                        \n    def expected_percentile_ranking(self):\n        def split_set(users, contents, splits):\n            output = list()\n            length = len(users) / splits\n            for i in range(splits):\n                output.append(str(i), [users[length * i: length * i + 1], contents[length * i: length * i + 1]])\n                \n            return output\n        \n        def process_chunk(chunk):\n            # each chunk contains a list of users and items.\n            subtrain = chunk[0]\n            users = chunk[1]\n            contents = chunk[2]\n            \n            for i in range(len(users)):\n                if i > 5000:\n                    break\n\n                try:\n                    accuracy.append(self.get_rank(users[i], contents[i], user_vector = self.user_dict[users[i]]) / self.num_items)\n                except:\n                    pass\n\n                if i % 1000 == 0:\n                    print(f\"Subtrain: {subtrain}. Reporting iteration: {i} of {len(users)}.\")\n        \n        accuracy = list()\n        mar = 0   \n        self.not_testable = 0        \n        users = self.MF.test_set.idUser.values\n        contents = self.MF.test_set.fullId.values\n\n        self.user_dict = self.MF.get_user_vectors(users)\n\n        # Multiprocessing unit\n        num_cores = 4\n        pool = Pool(num_cores)\n        all_chunks = expected_percentile_ranking.split_set(users, contents, num_cores)\n        result = pool.map(process_chunk, all_chunks)\n\n        #mar = np.mean(accuracy)\n\n        return result\n                        ","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implicit Alternating Least Squares Model Koren 2008.\nImplicit = Implicit(941, \"kaggle\")\n#Implicit.alternating_least_squares(iterations = 15, factors = 40, i_lambda = 0.15, alpha = 20)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Factorization Johnson 2014.\nLogistic = Implicit(941, \"kaggle\")\nLogistic.logistic_factorization(iterations = 15, factors = 60, i_lambda = 0.15, learning_parameter = 30)","execution_count":15,"outputs":[{"output_type":"stream","text":"100%|██████████| 15/15 [00:50<00:00,  3.35s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mar, accuracy = Implicit.expected_percentile_ranking()\nmar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Analysis = Studio(941, \"kaggle\", Implicit)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Analysis.run_test_als(v_iterations = [10], v_factors = [10, 20], v_lambdas = [0.1, 0.05, 0.15, 0.3], v_alphas = [10, 20, 40, 100])","execution_count":27,"outputs":[{"output_type":"stream","text":"iterations-10_factors-10_lambda-0.1_alpha-10\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=10), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34948daca1ed46ce8ba38b14325320fb"}},"metadata":{}},{"output_type":"stream","text":"\nSolved iteration: 0. That's about 0.0%.\nSolved iteration: 242. That's about 6.05%.\nSolved iteration: 484. That's about 12.1%.\nSolved iteration: 726. That's about 18.15%.\nSolved iteration: 968. That's about 24.2%.\nSolved iteration: 1210. That's about 30.25%.\nSolved iteration: 1452. That's about 36.3%.\nSolved iteration: 1694. That's about 42.35%.\nSolved iteration: 1936. That's about 48.4%.\nSolved iteration: 2178. That's about 54.45%.\nSolved iteration: 2420. That's about 60.5%.\nSolved iteration: 2662. That's about 66.55%.\nSolved iteration: 2904. That's about 72.6%.\nSolved iteration: 3146. That's about 78.65%.\nSolved iteration: 3388. That's about 84.7%.\nSolved iteration: 3630. That's about 90.75%.\nSolved iteration: 3872. That's about 96.8%.\n","name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"'tuple' object has no attribute 'append'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-34007a3fce21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAnalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test_als\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_lambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-b81869d59c95>\u001b[0m in \u001b[0;36mrun_test_als\u001b[0;34m(self, v_iterations, v_factors, v_lambdas, v_alphas)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mmodel_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model_als-iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_acc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'append'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training matrix works!\nidUser = 298\nidContent = \"1_16946\"\nindexUser = np.where(Factorizer.user_ids==idUser)[0][0]\nindexContent = np.where(Factorizer.content_ids==idContent)[0][0]\nFactorizer.training_set_csr.toarray()[indexUser][indexContent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.500 its, als\n0.10589308363675712\n# 15000 its, als\n0.11361161798583706","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}