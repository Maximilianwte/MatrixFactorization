{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast als by flipboard\n",
    "import implicit\n",
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((Factorizer.training_set*alpha).astype('double'), factors=20, regularization = 0.1, iterations = 50)\n",
    "user_vecs[0,:].dot(item_vecs.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix_Factorizer:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def read_matrices_small(self):        \n",
    "        #self.matrix_asFrame = pd.read_excel(\"../data_500_full.xlsx\")\n",
    "        self.matrix = sparse.load_npz(\"../ratings_matrix_500_full.npz\")\n",
    "        \n",
    "        self.num_users, self.num_items = self.matrix.shape\n",
    "        \n",
    "    def read_matrices_full(self, size):        \n",
    "        self.matrix_asFrame = pd.read_excel(f\"../data_10000_{size}.xlsx\")\n",
    "        #self.matrix_asFrame = self.matrix_asFrame.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "        self.matrix = sparse.load_npz(f\"../ratings_matrix_10000_{size}.npz\")\n",
    "        \n",
    "        self.num_users, self.num_items = self.matrix.shape\n",
    "    \n",
    "    def read_matrices_kaggle(self, size):\n",
    "        self.matrix_asFrame = pd.read_excel(f\"/kaggle/input/data_10000_{size}.xlsx\")\n",
    "        self.matrix = sparse.load_npz(f\"/kaggle/input/ratings_matrix_10000_{size}.npz\")\n",
    "        \n",
    "        self.num_users, self.num_items = self.matrix.shape\n",
    "        \n",
    "    def save_model(self, user_vec, item_vec, additional = \"\"):\n",
    "        time_int = int(time.time())\n",
    "        sparse.save_npz(f\"../saved_models/user_vec_{time_int}_{additional}\", user_vec, compressed=True)\n",
    "        sparse.save_npz(f\"../saved_models/item_vec_{time_int}_{additional}\", item_vec, compressed=True)\n",
    "        \n",
    "    def load_model(self, time):\n",
    "        user_vec = sparse.load_npz(f\"../saved_models/user_vec_{time}.npz\")\n",
    "        item_vec = sparse.load_npz(f\"../saved_models/item_vec_{time}.npz\")\n",
    "        \n",
    "        return user_vec, item_vec\n",
    "    \n",
    "    def create_training_data(self, percent_test_set = 0.15):\n",
    "        training_set = self.matrix.copy()\n",
    "        positive_indices = training_set.nonzero()\n",
    "        positive_values = list(zip(positive_indices[0], positive_indices[1]))\n",
    "        test_set = self.matrix.copy()\n",
    "        # The test set should only measure, if the item is going to be bought, not the frequency.\n",
    "        test_set[test_set != 0] = 1\n",
    "        random.seed(self.seed)\n",
    "        n_samples = int(percent_test_set*len(positive_values))\n",
    "        samples = random.sample(positive_values, n_samples)\n",
    "        # Put all chosen samples into lists.\n",
    "        user_indices = [index[0] for index in samples]\n",
    "        item_indices = [index[1] for index in samples]\n",
    "        # All users and items that are used in the test set, should be unviewed in the training set.\n",
    "        training_set[user_indices, item_indices] = 0\n",
    "        training_set.eliminate_zeros()\n",
    "        \n",
    "        self.training_set = training_set\n",
    "        self.test_set = test_set\n",
    "        self.indices = list(set(user_indices))\n",
    "        \n",
    "    def alternating_least_squares(self, k, alpha, v_lambda, n):\n",
    "        # K - Latent Factors, Alpha - Learning rate, V_lambda - Regulation, N - Iterations\n",
    "        training_set = self.training_set\n",
    "        test_set = self.test_set\n",
    "        \n",
    "        # Starting the confidence matrix\n",
    "        confidence = (alpha*training_set)\n",
    "        n_user = confidence.shape[0]\n",
    "        n_items = confidence.shape[1]\n",
    "        \n",
    "        X = sparse.csr_matrix(np.random.RandomState(self.seed).normal(size = (n_user, k)))\n",
    "        Y = sparse.csr_matrix(np.random.RandomState(self.seed).normal(size = (n_items, k)))\n",
    "        # sparse.eye is creating the einheitsmatrix with the specific value on the diagonal\n",
    "        X_e = sparse.eye(n_user)\n",
    "        Y_e = sparse.eye(n_items)\n",
    "        # The regulization term\n",
    "        lambda_e = v_lambda * sparse.eye(k)\n",
    "        \n",
    "        i = 0\n",
    "        length = n * n_user\n",
    "        report_step = int(length/30)\n",
    "        \n",
    "        for iteration in range(n):\n",
    "            Y_tY = Y.T.dot(Y)\n",
    "            X_tX = X.T.dot(X)\n",
    "            \n",
    "            for user in range(n_user):\n",
    "                confidence_user = confidence[user,:].toarray()\n",
    "                preference = confidence_user.copy()\n",
    "                preference[preference != 0] = 1\n",
    "                C_uI = sparse.diags(confidence_user, [0])\n",
    "                y_tC_uIY = Y.T.dot(C_uI).dot(Y)\n",
    "                y_tC_up_u = Y.T.dot(C_uI + Y_e).dot(preference.T)\n",
    "                # Minimizing X_u, keeping Y_i stable.\n",
    "                X[user] = spsolve(Y_tY + y_tC_uIY + lambda_e, y_tC_up_u)\n",
    "\n",
    "\n",
    "                # Progress Status\n",
    "                i += 1\n",
    "                if i % report_step == 0:\n",
    "                    print(f\"This is iteration: {i} of {length}.\")\n",
    "                \n",
    "                for item in range(n_items):\n",
    "                    confidence_item = confidence[:,item].T.toarray()\n",
    "                    preference = confidence_item.copy()\n",
    "                    preference[preference != 0] = 1\n",
    "                    C_iI = sparse.diags(confidence_item, [0])\n",
    "                    x_tC_iTX = X.T.dot(C_iI).dot(X)\n",
    "                    x_tC_iP_i = X.T.dot(C_iI + X_e).dot(preference.T)\n",
    "                    # Minimizing Y_i, keeping X_u stable.\n",
    "                    Y[item] = spsolve(X_tX + x_tC_iTX + lambda_e, x_tC_iP_i)\n",
    "                    \n",
    "        return X, Y.T\n",
    "    \n",
    "    def mean_percentage_ranking(self, user_vector, item_vector):\n",
    "        # for each item in the test set that was bought, check how hight it is in our predictions.\n",
    "        # 1. sort all calculated ratings\n",
    "        # 2. go through each bought one in the test set and check where we predicted it (in percent)\n",
    "        indices = self.indices\n",
    "        \n",
    "        for user_index in indices:\n",
    "            print(np.sort(user_vector[user_index,:].dot(item_vector).toarray()[0,:])[::-1])\n",
    "    \n",
    "    def test_parameters(self, in_k, in_alpha, in_v_lambda, in_n):\n",
    "        all_user_vecs = list()\n",
    "        all_item_vecs = list()\n",
    "        \n",
    "        length = len(in_k) * len(in_alpha) * len(in_v_lambda) * len(in_n)\n",
    "        iteration = 1\n",
    "        \n",
    "        for n in in_n:\n",
    "            for k in in_k:\n",
    "                for alpha in in_alpha:\n",
    "                    for v_lambda in in_v_lambda:\n",
    "                        user_vecs, item_vecs = self.alternating_least_squares(k=k, alpha=alpha, v_lambda=v_lambda, n=n)\n",
    "                        all_user_vecs.append(user_vecs)\n",
    "                        all_item_vecs.append(item_vecs)\n",
    "                        Factorizer.save_model(user_vecs, item_vecs, f\"k={k}_alpha={alpha}_lambda={v_lambda}_n={n}\")\n",
    "\n",
    "                        print(f\"Solved set: {iteration} of {length}. Parameters were K={k}, alpha={alpha}, lambda={v_lambda} and n={n}.\")\n",
    "                        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Factorizer = Matrix_Factorizer(941)\n",
    "Factorizer.read_matrices_full(\"extra_small\")\n",
    "Factorizer.create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Kaggle Kernels\n",
    "Factorizer = Matrix_Factorizer(941)\n",
    "Factorizer.read_matrices_kaggle(\"small\")\n",
    "Factorizer.create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one model\n",
    "user_vecs, item_vecs = Factorizer.alternating_least_squares(k=40, alpha=15, v_lambda=0.1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models with a set of parameters\n",
    "Factorizer.test_parameters(in_k=[10,20,50], in_alpha=[10], in_v_lambda=[0.1], in_n=[5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Factorizer.matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vecs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vecs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11300903804113793"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.max(user_vecs[0,:].dot(item_vecs).toarray()[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "saved_models = [\"1568052491_k=10_alpha=15_lambda=0.1_n=5\", \"1568079022_k=20_alpha=15_lambda=0.1_n=5\", \"1568108649_k=50_alpha=15_lambda=0.1_n=5\"]\n",
    "user_vecs, item_vecs = Factorizer.load_model(saved_models[0])\n",
    "#Factorizer.mean_percentage_ranking(user_vecs, item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
