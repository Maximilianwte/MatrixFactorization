{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.sparse as sparse\nimport os\nimport implicit\nimport time\nfrom IPython.display import FileLink\nfrom multiprocessing import Pool","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Implicit:\n    def __init__(self, seed = 941, environment = \"offline\"):\n        self.seed = seed\n        self.not_testable = 0\n        \n        if environment == \"kaggle\":\n            self.read_matrices_timesplit(\"/kaggle/input/\")\n        else:\n            self.read_matrices_timesplit(\"../\")\n        \n    def read_matrices_timesplit(self, path):\n        self.training_set_csr = sparse.load_npz(f\"{path}/ratings_matrix_csr.npz\")\n        self.training_set_coo = sparse.load_npz(f\"{path}/ratings_matrix_coo.npz\")\n        \n        self.user_ids = np.load(f\"{path}/user_ids.npy\")\n        self.content_ids = np.load(f\"{path}/content_ids.npy\")\n        \n        self.test_set = pd.read_pickle(f\"{path}/test_set.pkl\", compression=\"zip\")\n\n        self.num_users, self.num_items = self.training_set_csr.shape\n    \n    def alternating_least_squares(self, iterations, factors, i_lambda, alpha):\n        self.model = implicit.als.AlternatingLeastSquares(factors=factors, regularization = i_lambda, iterations = iterations)\n        self.model.fit((self.training_set_csr*alpha).astype('double'))\n                \n    def logistic_factorization(self, iterations, factors, i_lambda, learning_parameter):\n        self.model = implicit.lmf.LogisticMatrixFactorization(factors, learning_parameter, i_lambda, iterations = iterations)\n        self.model.fit((self.training_set_coo).astype('double'))\n    \n    def predict_user_byIndex(self, index):\n        return self.model.item_factors[index].dot(self.model.user_factors.T)\n    \n    def get_user_vectors(self, users):\n        user_dict = {\"user\": \"values\"}\n        \n        for i in range(len(users)):\n            try:\n                user_indx = np.where(self.user_ids==users[i])[0][0]\n                user_dict[users[i]] = self.predict_user_byIndex(user_indx)\n            except:\n                self.not_testable += 1\n                \n        return user_dict\n    \n    def get_rank(self, user, item, user_vector):\n        item_indx = np.where(self.content_ids==item)[0][0]\n        \n        prob = user_vector[item_indx]\n        return np.where(np.sort(user_vector, kind=\"mergesort\")[:5000]==prob)[0][0]\n        # [::-1]\n\n    def expected_percentile_ranking(self):\n        self.stopExecute = False\n        accuracy = list()\n        mar = 0   \n        self.not_testable = 0        \n        users = self.test_set.idUser.values\n        contents = self.test_set.fullId.values\n        breakpoint = 4500\n        reportpoint = breakpoint if breakpoint < len(self.test_set.idUser) else len(self.test_set.idUser)\n        step = 4350 if 4350 < (breakpoint/4) else 1550\n        \n        user_dict = self.get_user_vectors(self.test_set.idUser.unique())\n        \n        for i in range(len(self.test_set.idUser)):\n            j = np.random.randint(len(self.test_set.idUser))\n            if i > breakpoint:\n                break\n\n            try:\n                accuracy.append(self.get_rank(users[j], contents[j], user_vector = user_dict[users[j]]) / 5000)\n            except:\n                pass\n\n            if i % step == 0:\n                print(f\"Solved iteration: {i}. That's about {np.round((i/reportpoint)*100,2)}%.\")\n\n        mar = np.mean(accuracy)\n            \n        return mar, accuracy","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Studio:\n    def __init__(self, seed, env, factorizer = \"\"):\n        self.counter = 0\n        self.seed = seed\n        self.env = env\n        \n        if factorizer == \"\":\n            self.MF = Implicit(seed, env)\n            self.MF.create_training_data()\n        else:\n            self.MF = factorizer\n            \n    def save_model(self, user_vec, item_vec, parameters = \"\"):\n        np.save(f\"user_vec_{self.counter}_{parameters}\", user_vec)\n        np.save(f\"item_vec_{self.counter}_{parameters}\", item_vec)\n        self.counter += 1\n        \n    def run_test_als(self, v_iterations, v_factors, v_lambdas, v_alphas):\n        model_acc = list()\n        \n        for it in v_iterations:\n            for factor in v_factors:\n                for in_lambda in v_lambdas:\n                    for alpha in v_alphas:\n                        print(f\"Starting Iteration: iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\")\n                        self.MF.alternating_least_squares(iterations = it, factors = factor, i_lambda = in_lambda, alpha = alpha)\n                        mar, accuracy = self.MF.expected_percentile_ranking()\n                        \n                        #self.save_model(self.MF.model.user_factors, self.MF.model.item_factors, f\"{time.time()}_iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\")\n                        model_acc.append([f\"model_als-iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}\", mar, accuracy])                        \n                        print(f\"Fishing up Iteration: iterations-{it}_factors-{factor}_lambda-{in_lambda}_alpha-{alpha}. Reported MAR: {mar}.\")\n        np.save(f\"model_acc_{time.time()}\", model_acc)              \n        return model_acc\n    \n    def run_test_log(self, v_iterations, v_factors, v_lambdas, v_learning):\n        model_acc = list()\n\n        for it in v_iterations:\n            for factor in v_factors:\n                for in_lambda in v_lambdas:\n                    for in_learning in v_learning:\n                        print(f\"Starting Iteration: iterations-{it}_factors-{factor}_lambda-{in_lambda}_learning_parameter-{in_learning}\")\n                        self.MF.logistic_factorization(iterations = it, factors = factor, i_lambda = in_lambda, learning_parameter = in_learning)\n                        mar, accuracy = self.MF.expected_percentile_ranking()\n\n                        #self.save_model(self.MF.model.user_factors, self.MF.model.item_factors, f\"{time.time()}_iterations-{it}_factors-{factor}_lambda-{in_lambda}_learning_parameter-{learning_parameter}\")\n                        model_acc.append([f\"model_als-iterations-{it}_factors-{factor}_lambda-{in_lambda}_learning_parameter-{in_learning}\", mar, accuracy])                        \n                        print(f\"Fishing up Iteration: iterations-{it}_factors-{factor}_lambda-{in_lambda}_learning_parameter-{in_learning}. Reported MAR: {mar}.\")\n        np.save(f\"model_acc_{time.time()}\", model_acc)              \n        return model_acc\n                        \n    def expected_percentile_ranking(self):\n        def split_set(users, contents, splits):\n            output = list()\n            length = int(len(users) / splits)\n            for i in range(splits):\n                if i != (splits -1):\n                    output.append([str(i+1), users[length * i: length * (i + 1)], contents[length * i: length * (i + 1)]])\n                else:\n                    output.append([str(i+1), users[length * i:], contents[length * i:]])\n                \n            return output\n        \n        mar = 0   \n        self.not_testable = 0        \n        users = self.MF.test_set.idUser.values\n        contents = self.MF.test_set.fullId.values\n\n        self.user_dict = self.MF.get_user_vectors(self.MF.test_set.idUser.unique())\n\n        # Multiprocessing unit\n        num_cores = 4\n        pool = Pool(num_cores)\n        all_chunks = split_set(users, contents, num_cores)\n        result = pool.map(self.process_chunk, all_chunks)\n\n        #mar = np.mean(accuracy)\n\n        return result\n    \n    def process_chunk(self, chunk):\n        # each chunk contains a list of users and items.\n        accuracy = list()\n        subtrain = chunk[0]\n        users = chunk[1]\n        contents = chunk[2]\n\n        print(f\"Subtrain: {subtrain}. Reporting running.\")\n\n        for i in range(len(users)):\n            if i > 3000:\n                break\n\n            try:\n                # !!!!!!!\n                # This model doesnt work, because self.user_dict will be undefined. Aggregate the user_dict somehow in the all_chunks.\n                # !!!!!!!!!!!\n                accuracy.append(self.MF.get_rank(users[i], contents[i], user_vector = self.user_dict[users[i]]) / self.MF.num_items)\n            except:\n                pass\n\n            if i % 1000 == 0:\n                print(f\"Subtrain: {subtrain}. Reporting iteration: {i} of {len(users)}.\")\n                \n        return accuracy \n                        ","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implicit Alternating Least Squares Model Koren 2008.\nModel_Implicit = Implicit(941, \"kaggle\")\nModel_Implicit.alternating_least_squares(iterations = 25, factors = 60, i_lambda = 0.1, alpha = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Factorization Johnson 2014.\nLogistic = Implicit(941, \"kaggle\")\nLogistic.logistic_factorization(iterations = 1, factors = 1, i_lambda = 0.1, learning_parameter = 0.005)","execution_count":67,"outputs":[{"output_type":"stream","text":"100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mar, accuracy = Model_Implicit.expected_percentile_ranking()\nmar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Analysis = Studio(941, \"kaggle\", Logistic)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_accuracy = Analysis.run_test_als(v_iterations = [20], v_factors = [40], v_lambdas = [0.1], v_alphas = [5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_accuracy = Analysis.run_test_log(v_iterations = [20], v_factors = [40], v_lambdas = [0.1], v_learning = [0.01, 0.1, 0.2, 1])","execution_count":null,"outputs":[{"output_type":"stream","text":"Starting Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.01\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 20/20 [00:55<00:00,  2.79s/it]\n","name":"stderr"},{"output_type":"stream","text":"Solved iteration: 0. That's about 0.0%.\nSolved iteration: 1550. That's about 34.44%.\nSolved iteration: 3100. That's about 68.89%.\nFishing up Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.01. Reported MAR: 0.6041.\nStarting Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.1\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 20/20 [00:51<00:00,  2.56s/it]\n","name":"stderr"},{"output_type":"stream","text":"Solved iteration: 0. That's about 0.0%.\nSolved iteration: 1550. That's about 34.44%.\nSolved iteration: 3100. That's about 68.89%.\nFishing up Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.1. Reported MAR: 0.5609999999999999.\nStarting Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.2\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 20/20 [00:49<00:00,  2.49s/it]\n","name":"stderr"},{"output_type":"stream","text":"Solved iteration: 0. That's about 0.0%.\nSolved iteration: 1550. That's about 34.44%.\nSolved iteration: 3100. That's about 68.89%.\nFishing up Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-0.2. Reported MAR: 0.53832.\nStarting Iteration: iterations-20_factors-40_lambda-0.1_learning_parameter-1\n","name":"stdout"},{"output_type":"stream","text":" 30%|███       | 6/20 [00:14<00:34,  2.48s/it]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training matrix works!\nidUser = 298\nidContent = \"1_16946\"\nindexUser = np.where(Factorizer.user_ids==idUser)[0][0]\nindexContent = np.where(Factorizer.content_ids==idContent)[0][0]\nFactorizer.training_set_csr.toarray()[indexUser][indexContent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_Implicit.model.user_factors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download Links"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"./model_acc.npy\"> Download Accuracy Numpy File</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"directory = list()\n\ndef get_directory():\n    for filename in os.listdir(\"./\"):\n        if \".ipynb\" not in filename:\n            directory.append(filename)\n            #input(\"Press Enter to continue...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_directory()\nlen(directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FileLink(r'model_acc.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate for each file in the directory the download link manually.\nFileLink(directory[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 500 reverse\n# 30\n0.5104545454545455\n# 3\n0.4364545454545455\n# 5\n0.4955\n#60\n0.49937499999999996","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5000 normal\n# 80\n0.40148333333333336\n# 5\n0.6035428571428572","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}